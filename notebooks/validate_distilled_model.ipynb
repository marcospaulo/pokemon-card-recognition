{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Distilled Student Model (Stage 2)\n",
    "\n",
    "This notebook validates the ConvNeXt-Tiny student model after knowledge distillation training.\n",
    "\n",
    "**Training Job:** pytorch-training-2026-01-11-09-27-50-349\n",
    "\n",
    "**Model:** s3://sagemaker-us-east-2-943271038849/pytorch-training-2026-01-11-09-27-50-349/output/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "import boto3\n",
    "import tarfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model artifact\n",
    "s3 = boto3.client('s3')\n",
    "bucket = 'sagemaker-us-east-2-943271038849'\n",
    "key = 'pytorch-training-2026-01-11-09-27-50-349/output/model.tar.gz'\n",
    "local_path = '/tmp/stage2_model.tar.gz'\n",
    "\n",
    "print(\"Downloading model from S3...\")\n",
    "s3.download_file(bucket, key, local_path)\n",
    "print(f\"Downloaded to {local_path}\")\n",
    "\n",
    "# Extract\n",
    "extract_dir = '/tmp/stage2_model'\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "with tarfile.open(local_path, 'r:gz') as tar:\n",
    "    tar.extractall(extract_dir)\n",
    "\n",
    "print(\"\\nExtracted files:\")\n",
    "for f in os.listdir(extract_dir):\n",
    "    size_mb = os.path.getsize(os.path.join(extract_dir, f)) / (1024**2)\n",
    "    print(f\"  {f}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Training Metrics from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint to see training metrics\n",
    "checkpoint_path = os.path.join(extract_dir, 'student_stage2_checkpoint.pt')\n",
    "\n",
    "# Need to define WarmupCosineScheduler class first to load checkpoint\n",
    "class WarmupCosineScheduler:\n",
    "    \"\"\"Placeholder to allow checkpoint loading\"\"\"\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "    print(\"=== Stage 2 Training Results ===\")\n",
    "    print(f\"Final Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"Cosine Similarity: {checkpoint.get('cosine_similarity', 'N/A'):.4f}\")\n",
    "    print(f\"Stage: {checkpoint.get('stage', 'N/A')}\")\n",
    "    print(f\"\\nCheckpoint keys: {list(checkpoint.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load checkpoint metadata: {e}\")\n",
    "    print(\"This is okay - we can still validate the model weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Student Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, model_name='convnext_tiny', embedding_dim=768, num_classes=17592):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=False, num_classes=0)\n",
    "        backbone_dim = self.backbone.num_features\n",
    "        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.arcface = nn.Linear(embedding_dim, num_classes, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embeddings = self.projector(features)\n",
    "        embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = StudentModel(\n",
    "    model_name='convnext_tiny',\n",
    "    embedding_dim=768,\n",
    "    num_classes=17592\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created on {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights (EMA)\n",
    "model_path = os.path.join(extract_dir, 'student_stage2.pt')\n",
    "state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# Strip _orig_mod. prefix if present (from torch.compile)\n",
    "if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "    state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "    print(\"Stripped _orig_mod. prefix from keys\")\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(\"âœ… Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample of validation data from S3\n",
    "# Note: Adjust this path based on where your validation data is stored\n",
    "val_data_s3 = 's3://pokemon-card-training-us-east-2/classification_dataset/val/'\n",
    "\n",
    "# For quick testing, let's use a local path or download subset\n",
    "# You'll need to adjust this based on your SageMaker Studio setup\n",
    "val_dir = '/opt/ml/input/data/val'  # or wherever you mount your data\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Check if validation data is available\n",
    "if os.path.exists(val_dir):\n",
    "    val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    print(f\"âœ… Loaded validation dataset: {len(val_dataset)} images, {len(val_dataset.classes)} classes\")\n",
    "else:\n",
    "    print(\"âš ï¸ Validation data not found. You'll need to:\")\n",
    "    print(f\"   1. Copy data from {val_data_s3}\")\n",
    "    print(f\"   2. Or mount the S3 bucket in SageMaker Studio\")\n",
    "    val_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_loader is not None:\n",
    "    @torch.no_grad()\n",
    "    def validate_model(model, val_loader, device):\n",
    "        model.eval()\n",
    "        all_embeddings = []\n",
    "        all_labels = []\n",
    "        \n",
    "        print(\"Generating embeddings...\")\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels)\n",
    "        \n",
    "        all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        \n",
    "        # Compute cosine similarity matrix\n",
    "        print(\"Computing similarities...\")\n",
    "        similarity_matrix = torch.mm(all_embeddings, all_embeddings.t())\n",
    "        \n",
    "        # Top-1 retrieval accuracy\n",
    "        similarity_matrix.fill_diagonal_(-1)  # Exclude self\n",
    "        top1_indices = similarity_matrix.argmax(dim=1)\n",
    "        top1_labels = all_labels[top1_indices]\n",
    "        top1_acc = (top1_labels == all_labels).float().mean().item()\n",
    "        \n",
    "        # Top-5 retrieval accuracy\n",
    "        top5_indices = similarity_matrix.topk(5, dim=1).indices\n",
    "        top5_labels = all_labels[top5_indices]\n",
    "        top5_acc = (top5_labels == all_labels.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        \n",
    "        # Average cosine similarity for same class\n",
    "        label_mask = all_labels.unsqueeze(0) == all_labels.unsqueeze(1)\n",
    "        label_mask.fill_diagonal_(False)\n",
    "        same_class_sim = similarity_matrix[label_mask].mean().item()\n",
    "        \n",
    "        # Average cosine similarity for different class\n",
    "        diff_class_sim = similarity_matrix[~label_mask].mean().item()\n",
    "        \n",
    "        return {\n",
    "            'top1_accuracy': top1_acc,\n",
    "            'top5_accuracy': top5_acc,\n",
    "            'same_class_similarity': same_class_sim,\n",
    "            'diff_class_similarity': diff_class_sim,\n",
    "            'separation': same_class_sim - diff_class_sim\n",
    "        }\n",
    "    \n",
    "    metrics = validate_model(model, val_loader, device)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ“Š VALIDATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Top-1 Retrieval Accuracy: {metrics['top1_accuracy']:.2%}\")\n",
    "    print(f\"Top-5 Retrieval Accuracy: {metrics['top5_accuracy']:.2%}\")\n",
    "    print(f\"Same Class Similarity:    {metrics['same_class_similarity']:.4f}\")\n",
    "    print(f\"Diff Class Similarity:    {metrics['diff_class_similarity']:.4f}\")\n",
    "    print(f\"Class Separation:         {metrics['separation']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping validation - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark inference speed\n",
    "import time\n",
    "\n",
    "batch_sizes = [1, 8, 16, 32, 64]\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    _ = model(dummy_input)\n",
    "\n",
    "print(\"\\nðŸ“ˆ INFERENCE SPEED (GPU)\")\n",
    "print(\"=\"*50)\n",
    "for bs in batch_sizes:\n",
    "    dummy_batch = torch.randn(bs, 3, 224, 224).to(device)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        _ = model(dummy_batch)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    throughput = (bs * 100) / elapsed\n",
    "    latency = (elapsed / 100) * 1000  # ms\n",
    "    \n",
    "    print(f\"Batch {bs:2d}: {throughput:6.1f} img/sec, {latency:5.1f} ms/batch\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "total, trainable = count_parameters(model)\n",
    "\n",
    "print(\"\\nðŸ” MODEL STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Architecture:      ConvNeXt-Tiny\")\n",
    "print(f\"Embedding Dim:     768\")\n",
    "print(f\"Total Parameters:  {total:,} ({total/1e6:.1f}M)\")\n",
    "print(f\"Trainable Params:  {trainable:,} ({trainable/1e6:.1f}M)\")\n",
    "print(f\"Model Size:        ~162 MB\")\n",
    "print(f\"Compression:       39x (from 1.1B teacher)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare with Teacher Model (Optional)\n",
    "\n",
    "If you want to compare student vs teacher performance, download the teacher model and run the same validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is optional - only if you want to compare with teacher\n",
    "# teacher_s3_path = 's3://pokemon-card-training-us-east-2/models/embedding/teacher/pokemon-card-dinov3-teacher-2026-01-10-13-31-34-937/output/model.tar.gz'\n",
    "# ... (load teacher and compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook validated the distilled ConvNeXt-Tiny student model trained with knowledge distillation from a DINOv3-ViT-Large teacher.\n",
    "\n",
    "**Key Metrics to Look For:**\n",
    "- **Top-1 Accuracy**: Should be >85% for good retrieval\n",
    "- **Class Separation**: Should be >0.3 for distinct embeddings\n",
    "- **Inference Speed**: Should be much faster than teacher (~10-50x)\n",
    "\n",
    "**Next Steps:**\n",
    "1. Export to ONNX for deployment\n",
    "2. Compile for Hailo-8L accelerator\n",
    "3. Deploy on Raspberry Pi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
